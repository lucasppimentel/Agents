{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc8d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente\n",
    "import os\n",
    "\n",
    "# Webscraper\n",
    "from utils import extract_article_text, find_relevant_links\n",
    "\n",
    "# Vector Database\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# AI\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Monitoramento\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Pipeline de Dados e Parsing\n",
    "from sqlalchemy import create_engine\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b2ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain setup\n",
    "summarizer = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"text\", \"source\"],\n",
    "    template=(\n",
    "        \"Resuma a not√≠cia a seguir de forma sucinta:\\n\\n\"\n",
    "        \"{text}\\n\\n\"\n",
    "        \"Resumo:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# CallbackHandler do LangFuse (par√¢metros no Env)\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Inst√¢ncia da IA\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa00c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites do webscraper\n",
    "news_sites = [\n",
    "    \"https://valor.globo.com/\",\n",
    "    \"https://www.folha.uol.com.br/\",\n",
    "    \"https://exame.com/\",\n",
    "    \"https://oglobo.globo.com/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando a vector database com o documento de ITR\n",
    "path_chroma = \"./02. Databases/chroma_db\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory=path_chroma, embedding_function=embeddings, collection_name=\"MyColletion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e8480",
   "metadata": {},
   "source": [
    "# 01. Extract\n",
    "\n",
    "- Implementa√ß√£o do WebScraper das not√≠cias\n",
    "- Resumo simples feito pela IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92247fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de resumos\n",
    "summaries = list()\n",
    "\n",
    "for site in news_sites:\n",
    "    print(f\"\\nüîç Procurando not√≠cias em: {site}\")\n",
    "    links = find_relevant_links(site)\n",
    "\n",
    "    for link in links:\n",
    "        print(f\"üì∞ Lendo: {link}\", end=\" ...\")\n",
    "        article, title = extract_article_text(link)\n",
    "\n",
    "        if article and len(article.split()) > 100:\n",
    "            print(\"Criando Resumo\")\n",
    "            summary = summarizer.invoke(template.invoke({\"text\": article[:10000], \"source\": link}), config={\"callbacks\": [langfuse_handler]}).content\n",
    "            summaries.append((link, summary, title))\n",
    "        else: print()\n",
    "\n",
    "print(\"\\nüìä RESUMOS RELEVANTES PARA O MERCADO:\\n\")\n",
    "for i, (link, summary, title) in enumerate(summaries, 1):\n",
    "    print(f\"{i}. {link}\\n{summary}\\n{'-'*80}\")\n",
    "\n",
    "# Armazenar output em Data Frame\n",
    "df_summaries = pd.DataFrame(data=summaries, columns=[\"url\", \"summary\", \"Titulo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ba867",
   "metadata": {},
   "source": [
    "# 02. Transform\n",
    "\n",
    "- Classifica√ß√£o de impacto e relev√¢ncia com RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8dca81",
   "metadata": {},
   "source": [
    "### RAG e P√≥s-RAG\n",
    "\n",
    "1. Primeira extra√ß√£o de Chunks do documento da Petrobr√°s\n",
    "2. Remo√ß√£o de ru√≠do e filtro de chunks relevantes com LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1b22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "<Instru√ß√£o>\n",
    "Voc√™ esta ajudando a analisar se o resumo da not√≠cia √© relevante para a estrat√©gia da empresa ou n√£o\n",
    "<Instru√ß√£o>\n",
    "\n",
    "<Resumo da Not√≠cia>\n",
    "{summary}\n",
    "<Resumo da Not√≠cia>\n",
    "\n",
    "Abaixo s√£o chunks de documentos internos da empresa.\n",
    "\n",
    "<Chunks>\n",
    "{chunks}\n",
    "<Chunks>\n",
    "\n",
    "Reordene os chunks do mais √∫til ao menos √∫til para determinar a relev√¢ncia para a estrat√©gia da empresa. Responda somente com os mais √∫teis.\n",
    "\"\"\"\n",
    "\n",
    "pos_rag_template = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"chunks\"],\n",
    "    template=query\n",
    ")\n",
    "\n",
    "def CleanRetrievals(resumo, chunks_):\n",
    "    docs = \"\\n\\n\".join([x.page_content for i, x in enumerate(chunks_)])\n",
    "\n",
    "    response = llm.invoke(\n",
    "        pos_rag_template.invoke({\"summary\":resumo, \"chunks\":docs}), \n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    ).content\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c474b01",
   "metadata": {},
   "source": [
    "### Classifica√ß√£o das not√≠cias com LLM\n",
    "\n",
    "Chain-of-Thought + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49943e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "<Instru√ß√£o>\n",
    "Voc√™ √© um especialista financeiro especializado na Petrobr√°s. Leia o resumo da not√≠cia e, com base no contexto fornecido, avalie se ela √© relevante ou n√£o para a Petrobr√°s. Apresente seu racioc√≠nio passo a passo com base em elementos do contexto e da not√≠cia. Em seguida, forne√ßa os seguintes crit√©rios:\n",
    "Relev√¢ncia: [\"Relevante\" ou \"N√£o Relevante\"]\n",
    "Impacto: [\"Positivo\", \"Negativo\" ou \"Neutro\"]\n",
    "N√≠vel: [\"Alto\", \"M√©dio\" ou \"Baixo\"]\n",
    "Key-Takeaways: [Pontos chave do seu racioc√≠nio]\n",
    "<Instru√ß√£o>\n",
    "\n",
    "<Vari√°veis>\n",
    "Contexto: Cont√©m trechos do Formul√°rio de Refer√™ncia da Petrobr√°s, que cont√©m informa√ß√µes sobre riscos e sobre a estrat√©gia da empresa.\n",
    "Not√≠cia: Resumo de uma not√≠cia.\n",
    "<Vari√°veis>\n",
    "\n",
    "<Contexto>\n",
    "{context}\n",
    "<Contexto>\n",
    "\n",
    "<Formato da Resposta>\n",
    "**An√°lise e Racioc√≠nio**: [Explique passo a passo como a not√≠cia se conecta (ou n√£o) com o contexto da Petrobr√°s, mencionando poss√≠veis impactos financeiros, estrat√©gicos, regulat√≥rios ou reputacionais.]\n",
    "\n",
    "Relev√¢ncia: \"Relevante\" ou \"N√£o Relevante\"\n",
    "Impacto: \"Positivo\", \"Negativo\" ou \"Neutro\"\n",
    "N√≠vel: \"Alto\", \"M√©dio\" ou \"Baixo\"\n",
    "Key-Takeaways: \n",
    "- Primeiro takeaway do seu racic√≠nio, em poucas palavras. Adicione um ou mais, se for relevante.\n",
    "- Segundo takeaway do seu racic√≠nio, em poucas palavras. Se for relevante\n",
    "- Tericeiro takeaway do seu racic√≠nio, em poucas palavras. Se for relevante\n",
    "<Formato da Resposta>\n",
    "\n",
    "<Not√≠cia>\n",
    "{summary}\n",
    "<Not√≠cia>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"summary\"],\n",
    "    template=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b1a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 13 ...M√©dio Positivo\n",
      "2 / 13 ...M√©dio Neutro\n",
      "3 / 13 ...Alto Negativo\n",
      "4 / 13 ...N√£o Relevante\n",
      "5 / 13 ...N√£o Relevante\n",
      "6 / 13 ...N√£o Relevante\n",
      "7 / 13 ...N√£o Relevante\n",
      "8 / 13 ...Alto Negativo\n",
      "9 / 13 ...N√£o Relevante\n",
      "10 / 13 ...N√£o Relevante\n",
      "11 / 13 ...Alto Positivo\n",
      "12 / 13 ...Alto Negativo\n",
      "13 / 13 ...N√£o Relevante\n"
     ]
    }
   ],
   "source": [
    "for i, summary in df_summaries.iterrows():\n",
    "    print(f\"{i+1} / {len(df_summaries)}\", end=\" ...\")\n",
    "\n",
    "    # Retrieval + P√≥s-Retrieval\n",
    "    docs = vectorstore.similarity_search(summary[\"summary\"], k=20)\n",
    "    filtered = [doc for doc in docs if doc.metadata.get(\"category\") in [\"Empresa e estrutura\", \"Opera√ß√µes e mercado\", \"Riscos e conformidade\", \"ESG e sustentabilidade\"]]\n",
    "    context = CleanRetrievals(summary[\"summary\"], filtered)\n",
    "\n",
    "    # Classifica√ß√£o da not√≠cia\n",
    "    response = llm.invoke(\n",
    "        prompt_template.invoke({\"summary\":summary[\"summary\"], \"context\":context}), \n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    ).content\n",
    "\n",
    "    # Output Parsing\n",
    "    for variavel in [\"Relev√¢ncia\", \"Impacto\", \"N√≠vel\"]:\n",
    "        parsed = re.findall(fr'{variavel}:\\s*\\\"([^\\n]+)\\\"', response.replace(\"*\", \"\"))[0]\n",
    "\n",
    "        nfkd_form = unicodedata.normalize('NFKD', variavel).lower()\n",
    "        col = \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "        df_summaries.loc[i, col] = parsed\n",
    "\n",
    "    takeaways = response.split(\"Key-Takeaways:\")[-1]\n",
    "    df_summaries.loc[i, \"Takeaways\"] = takeaways\n",
    "    df_summaries.loc[i, \"Pensamento\"] = response\n",
    "    \n",
    "    print(f\"{df_summaries.loc[i, \"nivel\"]} {df_summaries.loc[i, \"impacto\"]}\" if df_summaries.loc[i, \"relevancia\"] == \"Relevante\" else df_summaries.loc[i, \"relevancia\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859e1f5",
   "metadata": {},
   "source": [
    "# 03. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e29204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_summaries_database = \"./02. Databases/Summaries/\"\n",
    "hoje = dt.datetime.now().strftime(format=\"%Y-%m-%d\")\n",
    "\n",
    "df_summaries[\"Date\"] = hoje\n",
    "\n",
    "if not os.path.exists(path_summaries_database):\n",
    "    os.makedirs(path_summaries_database)\n",
    "df_summaries.to_csv(path_summaries_database + f\"{hoje}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6cc3048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summaries.columns = list(map(lambda x: x.lower(), df_summaries.columns))\n",
    "df_summaries = df_summaries.rename(columns={\"titulo\": \"title\", \"pensamento\": \"ideia\", \"nivel\": \"relevancia\", \"relevancia\": \"flag\", \"flag\": \"relevancia\"})\n",
    "\n",
    "db_url = f'postgresql+psycopg2://{os.environ[\"POSTGRES_USER\"]}:{os.environ[\"POSTGRES_PASSWORD\"]}@localhost:5432/{os.environ[\"POSTGRES_DATABASE\"]}'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Append data to the existing table\n",
    "df_summaries.to_sql(name=\"summaries\", schema=\"ai\", con=engine, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
